{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17aac157",
   "metadata": {},
   "source": [
    "# Import packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3e37def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ef55291",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = pd.read_csv('/Users/michelle/Desktop/final-project/SMSSpamCollection.txt', sep = \"\\t\" ,names=[\"Label\", \"Message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18a5cec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Label                                            Message\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "print(spam.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f97ae35",
   "metadata": {},
   "source": [
    "# Convert the dataFrame into a List of tuples, each tuple(row) contains label or message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "609e26cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = []\n",
    "for index, row in spam.iterrows():\n",
    "    data_set.append((row['Message'], row['Label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d506cf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...', 'ham'), ('Ok lar... Joking wif u oni...', 'ham'), (\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\", 'spam'), ('U dun say so early hor... U c already then say...', 'ham'), (\"Nah I don't think he goes to usf, he lives around here though\", 'ham')]\n"
     ]
    }
   ],
   "source": [
    "print(data_set[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11887b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5572\n"
     ]
    }
   ],
   "source": [
    "print(len(data_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c133ff",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "## remove stopwords/ tokenization/ stemmer or lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea4131c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3911198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(document, stem=True):\n",
    "    'changes document to lower case, remove stopwords and lemmatizes/stems '\n",
    "    \n",
    "    # changes sentence to lower case\n",
    "    document = document.lower()\n",
    "    \n",
    "    # tokenize into words\n",
    "    words = word_tokenize(document)\n",
    "    \n",
    "    # remove stop words\n",
    "    words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "    \n",
    "    if stem:\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "    else:\n",
    "        words = [wordnet_lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "        \n",
    "    document = \" \".join(words)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "082eefd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/michelle/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27b815b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/michelle/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2b3accd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/michelle/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e965ec",
   "metadata": {},
   "source": [
    "## Perform preprocessing step on data_Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95275bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_set = []\n",
    "for (message, label) in data_set:\n",
    "    words_filtered = [e.lower() for e in preprocess(message, stem=False).split() if len(e) >=3]\n",
    "    messages_set.append((words_filtered, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5654e619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['jurong', 'point', 'crazy', 'available', 'bugis', 'great', 'world', 'buffet', '...', 'cine', 'get', 'amore', 'wat', '...'], 'ham'), (['lar', '...', 'joke', 'wif', 'oni', '...'], 'ham'), (['free', 'entry', 'wkly', 'comp', 'win', 'cup', 'final', 'tkts', '21st', 'may', '2005.', 'text', '87121', 'receive', 'entry', 'question', 'std', 'txt', 'rate', 'apply', '08452810075over18'], 'spam'), (['dun', 'say', 'early', 'hor', '...', 'already', 'say', '...'], 'ham'), (['nah', \"n't\", 'think', 'usf', 'live', 'around', 'though'], 'ham')]\n"
     ]
    }
   ],
   "source": [
    "print(messages_set[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac986e6",
   "metadata": {},
   "source": [
    "## Preparing to create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b1d3512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_in_messages(messages):\n",
    "    all_words = []\n",
    "    for (message, label) in messages:\n",
    "        all_words.extend(message)\n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69a809d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "293d184e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7995\n"
     ]
    }
   ],
   "source": [
    "word_features = get_word_features(get_words_in_messages(messages_set))\n",
    "print(len(word_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05acb54",
   "metadata": {},
   "source": [
    "## Preparing to create train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acff4235",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliceIndex = int((len(messages_set) * .8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ee2e142",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(messages_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1e7fa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_messages, test_messages = messages_set[:sliceIndex], messages_set[sliceIndex:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78a89da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_messages)\n",
    "len(test_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677a34de",
   "metadata": {},
   "source": [
    "## Preparing to create feature maps to train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "848460c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b95d48ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = nltk.classify.apply_features(extract_features, train_messages)\n",
    "testing_Set = nltk.classify.apply_features(extract_features, test_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b008509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  4457\n",
      "Test set size:  1115\n"
     ]
    }
   ],
   "source": [
    "print ( 'Training set size: ', len(training_set))\n",
    "print('Test set size: ', len(testing_Set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb01caf0",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a57b0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f381823",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f397062c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9928202827013687\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, training_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20d4c5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9757847533632287\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, testing_Set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c88ffa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification result:  ham\n"
     ]
    }
   ],
   "source": [
    "messages = 'Congratulations!! Youve been selected to provide feedback. As a recent walgreens music, we are offering you the opportunity to receive a $100 Gift Card for answereing'\n",
    "print('Classification result: ', classifier.classify(extract_features(messages.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e082348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "         contains(award) = True             spam : ham    =    182.1 : 1.0\n",
      "        contains(urgent) = True             spam : ham    =    138.2 : 1.0\n",
      "         contains(await) = True             spam : ham    =    103.1 : 1.0\n",
      "       contains(service) = True             spam : ham    =     84.1 : 1.0\n",
      "         contains(nokia) = True             spam : ham    =     72.4 : 1.0\n",
      "      contains(landline) = True             spam : ham    =     67.1 : 1.0\n",
      "           contains(100) = True             spam : ham    =     63.6 : 1.0\n",
      "      contains(delivery) = True             spam : ham    =     63.6 : 1.0\n",
      "           contains(txt) = True             spam : ham    =     61.9 : 1.0\n",
      "        contains(camera) = True             spam : ham    =     59.2 : 1.0\n",
      "        contains(expire) = True             spam : ham    =     59.2 : 1.0\n",
      "       contains(private) = True             spam : ham    =     59.2 : 1.0\n",
      "        contains(todays) = True             spam : ham    =     59.2 : 1.0\n",
      "         contains(video) = True             spam : ham    =     59.2 : 1.0\n",
      "        contains(latest) = True             spam : ham    =     56.6 : 1.0\n",
      "       contains(attempt) = True             spam : ham    =     51.4 : 1.0\n",
      "          contains(rate) = True             spam : ham    =     51.4 : 1.0\n",
      "        contains(reveal) = True             spam : ham    =     50.5 : 1.0\n",
      "      contains(customer) = True             spam : ham    =     49.7 : 1.0\n",
      "        contains(mobile) = True             spam : ham    =     49.5 : 1.0\n",
      "         contains(apply) = True             spam : ham    =     48.0 : 1.0\n",
      "        contains(caller) = True             spam : ham    =     46.1 : 1.0\n",
      "           contains(opt) = True             spam : ham    =     46.1 : 1.0\n",
      "          contains(quiz) = True             spam : ham    =     46.1 : 1.0\n",
      "          contains(draw) = True             spam : ham    =     44.6 : 1.0\n",
      "         contains(music) = True             spam : ham    =     43.4 : 1.0\n",
      "       contains(auction) = True             spam : ham    =     41.7 : 1.0\n",
      "          contains(club) = True             spam : ham    =     41.7 : 1.0\n",
      "           contains(del) = True             spam : ham    =     41.7 : 1.0\n",
      "           contains(box) = True             spam : ham    =     38.8 : 1.0\n",
      "        contains(orange) = True             spam : ham    =     38.6 : 1.0\n",
      "          contains(land) = True             spam : ham    =     38.2 : 1.0\n",
      "contains(congratulations) = True             spam : ham    =     37.3 : 1.0\n",
      "       contains(contact) = True             spam : ham    =     35.7 : 1.0\n",
      "          contains(info) = True             spam : ham    =     35.5 : 1.0\n",
      "          contains(comp) = True             spam : ham    =     32.9 : 1.0\n",
      "       contains(england) = True             spam : ham    =     32.9 : 1.0\n",
      "           contains(mat) = True             spam : ham    =     32.9 : 1.0\n",
      "          contains(user) = True             spam : ham    =     32.9 : 1.0\n",
      "        contains(select) = True             spam : ham    =     31.5 : 1.0\n",
      "        contains(arrive) = True             spam : ham    =     28.5 : 1.0\n",
      "           contains(dvd) = True             spam : ham    =     28.5 : 1.0\n",
      "           contains(rat) = True             spam : ham    =     28.5 : 1.0\n",
      "           contains(sex) = True             spam : ham    =     27.6 : 1.0\n",
      "          contains(cash) = True             spam : ham    =     26.6 : 1.0\n",
      "         contains(offer) = True             spam : ham    =     25.9 : 1.0\n",
      "           contains(win) = True             spam : ham    =     25.6 : 1.0\n",
      "        contains(colour) = True             spam : ham    =     25.4 : 1.0\n",
      "      contains(motorola) = True             spam : ham    =     25.0 : 1.0\n",
      "          contains(mins) = True             spam : ham    =     24.1 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(classifier.show_most_informative_features(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be51a88",
   "metadata": {},
   "source": [
    "## code is flowing better took time to figure out what I was missing in certain areas of the code\n",
    "## heading to label work\n",
    "## week 5 work\n",
    "## still preparing rest of work"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
